# ============================================
# RAG Image Expert - LLM Configuration
# ============================================
# This file supports ANY OpenAI-compatible LLM API
# Copy to .env and configure for your LLM provider

# --------------------------------------------
# Option 1: xAI Grok (Default)
# --------------------------------------------
XAI_API_KEY=your-grok-api-key-here
# AI_BASE_URL defaults to https://api.x.ai/v1
AI_MODEL=grok-4-1-thinking

# --------------------------------------------
# Option 2: OpenAI
# --------------------------------------------
# XAI_API_KEY=sk-your-openai-key-here
# AI_BASE_URL=https://api.openai.com/v1
# AI_MODEL=gpt-4

# --------------------------------------------
# Option 3: Local LLM - Ollama (RECOMMENDED AS FALLBACK)
# --------------------------------------------
# No API key needed for localhost!
# Use as fallback when api.x.ai is unreachable (DNS/network issues in WSL2)
#
# Quick setup:
# 1. Install: curl -fsSL https://ollama.com/install.sh | sh
# 2. Pull model: ollama pull llama3.2
# 3. Uncomment these lines:
#
# AI_BASE_URL=http://localhost:11434/v1
# AI_MODEL=llama3.2
# # XAI_API_KEY not required for localhost

# --------------------------------------------
# Option 4: Local LLM - LM Studio
# --------------------------------------------
# Download LM Studio from https://lmstudio.ai/
# Load a model and start the local server
#
# AI_BASE_URL=http://localhost:1234/v1
# AI_MODEL=local-model
# # XAI_API_KEY not required for localhost

# --------------------------------------------
# Option 5: Other OpenAI-Compatible APIs
# --------------------------------------------
# AI_BASE_URL=https://your-api-endpoint.com/v1
# AI_MODEL=your-model-name
# XAI_API_KEY=your-api-key

# --------------------------------------------
# Server Configuration (Optional)
# --------------------------------------------
# PORT=3000

# --------------------------------------------
# Session Management (Phase 2)
# --------------------------------------------
# Enable persistent session storage with token optimization
# When enabled: conversations saved to SQLite, 52% token reduction
# When disabled: stateless operation (backward compatible)
USE_DB_SESSIONS=false

# ============================================
# Phase 3: Image Generation & MCP Services
# ============================================

# --------------------------------------------
# Image Generation Provider (Choose ONE)
# --------------------------------------------
# Option 1: Fal.ai (RECOMMENDED - Faster & Cheaper)
# Get your token from https://fal.ai/dashboard/keys
# Pricing: ~$0.003/image (Flux Schnell), ~$0.04/image (Flux Pro)
# Speed: 2-5s (Schnell), 8-15s (Pro)
FAL_API_KEY=your-fal-api-key-here

# Option 2: Replicate (Alternative)
# Get your token from https://replicate.com/account/api-tokens
# Pricing: ~$0.003/image (Flux Schnell), ~$0.055/image (Flux Pro)
# Speed: 3-6s (Schnell), 10-20s (Pro)
# REPLICATE_API_TOKEN=your-replicate-token-here

# Models available (both providers):
# - flux-schnell (fast, 4 steps)
# - flux-dev (quality, 28 steps)
# - flux-pro (photorealistic, 50 steps)
# - flux-realism (Fal only - enhanced realism)
# - sdxl (classic Stable Diffusion)

# Option 3: Hugging Face (for Z-Image-Turbo image-to-image)
# Get your token from https://huggingface.co/settings/tokens
# Z-Image-Turbo: Fast image transformation model
# Use for: style transfer, image enhancement, creative variations
HUGGINGFACE_API_KEY=your-huggingface-token-here

# --------------------------------------------
# MCP Services (Optional - Auto-enabled if available)
# --------------------------------------------
# Memory Bank MCP: Remembers successful prompts, preferences, failures
# Context7 MCP: Fetches live documentation from GitHub/web
#
# These services auto-start via npx (no additional config needed)
# - Memory Bank: Persists generation history and user preferences
# - Context7: Auto-fetches live docs when query contains "latest", "new", etc.
#
# To disable: Services gracefully degrade if unavailable
